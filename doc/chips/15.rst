Chapel Incremental Compilation
==============================

Status
  Draft

Author
  Kushal Singh

Abstract
--------

Currently, re-compiling a Chapel program would mean re-compiling the parts that haven’t changed, including the functions that haven’t been altered since the last compilation. The idea behind this project is to utilise the already existing information that compiler already has to speed up re-compilation. The main steps in the project would be to start away with restructuring the way currently elements with same name are distinguished. The main goal would be to create a code cache which can be stored in as a part of the object file. We would like to utilise our code cache to avoid re-instantiation and recompilation for functions that are already present as a part of the code cache, and finally we want to be able to re-link the correct functions and symbols from the cache during the building of the executable.


Description
-----------

The idea of this project is to speed up re-compilation. A part of which revolves around hashing up the AST. The idea is to  hash the function nodes present in the AST and compare based on the hash value, if the function has been modified. The granularity at which we make this checks can be adjusted, for instance instead of per function basis approach, we can switch to per module based approach. Once we identify the places where our current AST differs, we can accordingly choose a plan of action. The main part of the project would be dealing with the function resolution. For  generic function which have been modified, we need to identify the locations at which the function got instantiated and all those instantiations will need to be re-instantiated again (and the current one’s being removed). If the function is not a generic function then such a case would not arise, and we would can work at the same resolution level. There might be certain extra cases which we might have to consider, for instance changing the return type of a function would need identifying all the locations where it is being called so that appropriate handling can be done. Some part of the hash computation and utilising the code cache would need some modifications in the resolve pass. The re-computation of the hash value and checking can also be done to some extent just after the resolve pass is complete. However, it may would in a bit of extra re-computation.

Once we are able to create a code cache, which we can store at the end of our object file (.o file). We can store a modified version of the AST, since we may not need the entire information present in the AST, but only a subset of it. This modified AST can then be stored in the .o file. Now, we would need to identify during our function resolution, if the cache already contains instance of our generic function, so that we need not re-instantiate a function that is already present in the cache. We can make appropriate changes during our resolve pass to identify if the cache already holds the instance, in which case it can be appropriately utilised.  A significant part of changes would be in the code generation part, where based on the code cache, we should not re-compile the functions which are already present in the code cache. This might also need some changes during the resolve pass, to see if a function has already been resolved previously, and is present in the cache. Code generation will have to be adjusted to appropriately handle these conditions. Lastly, we would need to adjust executable building to link in the symbols from the cache.

Since most of the changes that we make require us to identify the locations where a given function has been instantiated/called. We can utilise the underlying structure of our FnSymbol Class, which is used to represent all the methods and functions in a program. The class has different fields which get populated during the function resolution, some of which include instantiatedFrom ( set for functions that have been instantiated from a generic function), instantiationPoint (points to point in code which we are using as instantiation point for function resolution), and calledBy vector (which points to all of the caller expressions, that call the function). The calledBy vector is computed by compute_call_sites (which essentially builds a call graph for the entire program represented by the AST). The compute_call_sites function can be modified or utilised to find out the positions where the changes can be made. Other useful functions that can be utilised to avoid re-computation would include collectFnCalls which essentially collects all the CallExprs that are not primitive. Also, we need to identify if our function is generic or not and modify the action taken based it, we can also utilise getVisibleFunctions to help us identify this. So, a major part of the project would be to utilise this and other underlying information that is already present, and modify the logic to accommodate and take into the account the parts where utilising a code cache can be helpful. The ideals and goals were thought iteratively thought of, and re-structured during different phases of the project. Finally we came up with an approach (described in the next section), which essentially leads to an easier way to approach the issue in terms of understanding it, and starting from basic building blocks for the same.


Paths considered
----------------

The initial approach was to first start of with splitting the header file into smaller pieces, so that each module can compile independently. The goal was to enable each of the module files on which we depend to be able to compiled independently and then linked all together, as opposed to having everything getting compiled together all at once. We wanted to create a single header file per standard/internal module instead of putting all the declarations in chpl__header.h. The generated .c file would #include the module header for all the module it needs, as known via module use list. (using include-guards), special care needed for user modules. However this approach did have some issues , for instance , if the modification occurred to the declaration of the function, then the header would always have to be updated. Which would mean that files which relied on it would have to be updated too. Which essentially meant regenerating .o  for those even if we did not re-write the .c files. Another issue with this approach was finding whether a function has changed was a problem for both the function resolution and make binary step. So based on these issues and on further discussion a later step by step approach was discussed which has been described in the later half of the description, which was followed for the second half of the duration.

However interesting observation that we did come across during this part was that we can build up test cases based on which we can verify the correctness, since the changes aren’t interactive. The different flag changes can be handled by the test case system. Basic implementation can work without recombining flags, however a more refined and sophistication approach would have to take into account the different flags that come into play and how they affect each other. Now this feature is a purely performance based feature, we would not like to yield false positives and give incorrect results in cases when we shouldn't. Hence we need to also have a testing mechanism later on that would indicate whether the feature is working as expected. Since it would be linked to a cache, and based on that it would try to touch different parts of it , we can have a separate style for testing such a feature.

The planned way to check if a file foo.chpl has works as planned would be to have copies of foo.chpl all slightly modified. A file named foo.orig.chpl would be the initial never-before-compiled version. We’d copy it on top of foo.chpl after compiling once foo.orig.chpl (layered over it) and then try recompiling. In the process we can check which of the files were touched (in cache), and verify that the executable behaved correctly for the changes and the code that hadn’t been updated. The same process can be repeated for different versions of like foo.step2.chpl, .. foo.stepN.chpl.
This way we can have expected output for each of the modifications, and save changes for reproducibility. The following shows some different 2-3 lined programs all with slightly modified version of the same code

Sample foo.chpl versions : http://bit.ly/1YcIIhZ 

Now even though there was a slight mention about the compiler flags , there was a few more things related to it. The flags that are in play , do affect how the particular function goes through the compiler (--no-checks, --no-inline etc). Also, they do affect the end results and affect how they are used in the next compilation. One way to take this interplay of flags into account can be to have multiple versions of a function. (However the drawback is that based on the different combination of flags , this would lead to an exponential blowup). The other way (the better way to test would however include, saving the flag with which the function was compiled and then pay an algorithmic cost to determine how the function would respond to particular changes. This approach would be better since the total number of flags are finite.

There was also a bit of discussion about how we wanted to work based on a pass level granularity (multi-pass approach). Now there are at-least 7 passes which are (obtained via flags) to be taken into account, resolve, inlineFunctions, loopInvariantCodeMotion, copyPropagation, deadCode Elimination, scalarReplace and codegen etc, for which we would have to maintain state for.We can have way-points, e.g resolve’s check would handle the passes after resolve but before deadCodeElimination, and deadCodeElimination would handle everything from there to codegen. For correctness we need to respond to the changes that occur in these passes, and the way point option would be to avoid the cost of having to store information on a function for each individual pass.

However towards the end, was is the main approach / building blocks we have been trying to
follow :

1) Our header file, which is generated has usually static symbols etc, which make it difficult to link it into multiple places. So a part of the work would be to make sure that static keyword is removed to allow linking in from multiple places. The other part would be making sure that the chpl__header.h file actually has only the declarations and not the definitions. We have achieved the splitting of header file completely , however removing static keyword causes a performance drop in some of our benchmark programs. Hence we support it completely only with the use of --incremental flag. This also follows our initial design idea of trying to have things compile independently, rather than compiling everything at once.

2) The next part once we have a header file would be to link it into multiple different locations, this would require adding a #include to the user code to chpl__header.h and instead of including then in the _main.c  file (which causes one big .o file to get generated), we want separate .o files to be generated, which can then be later re-linked. Note that this generation of multiple .o files too is done only with the --incremental flag. Since currently in the normal compilation mode, we do add the 'static' keyword , which does not allow #including of header file at multiple places.

3) The next step once we have the different .o files would be to save the .c/.o files that haven't changed between two different compilation runs. The way to achieve this is to create a new directory like --savec does, and then based on that we start populating the cache. A point worth mentioning here is that along with different .o's and .c's, we would also try to store the values of different flags and environment variables for that run (this however can be done using the printchplenv mechanism). Essentially we would have two directories here, One which gets generated each time (like the tmp directory for --savec), and the other one which is at a fixed location. For the first run we simple copy the contents into the cache, for the next run we do a quick diff to find out the regions which haven't changed. A point worth mentioning is that our standard libraries are always compiled with the program instead of living in a complete precompiled state. But we want the diff's to be present only in the user part of the code, we would go with finding out if there are any diff's during different runs of the standard libraries, and once we are able to minimise that successfully, then we analyse how the program behaves with small modifications to the user code. This is mainly an understanding step, once we are able to successfully understand an analyse this we can easily move to more sophisticated analysis techniques.

4) Once we have this mechanism set up in place, we can then start detecting the modification to the files in the chapel compiler, we want to do some verification here first as a precautionary step to ensure that all the files we expect to be same are generated the same. This would include the following :

1. Detecting the changes to the compiler flags and environment variables
2. Detecting if the code has changed due to :

* Changes in it's body and declaration (includes arguments, return types  etc). This can be detected once we have the old AST representation.
* Changes in the function call, this would include identifying where the functions were called from (can utilise some of the existing machinery like compute_call_sites() here). This step however would be harder as the info for this is not populated till function resolution.
* New candidates to resolve against for a function call.

This would require marking the nodes which haven't changed, so that passes later than when this detection has occurred can benefit from it. There were a few simplifying assumptions however with this model (which may require more detailed analysis later on)

* Current assumption was that if there is a change in the standard and internal library, or change in the command line arguments or environment labels the cache would not be re-useable again. This however is not true in general, but would require a finer and more detailed analysis.

*  The idea was to start with a single module program and move on from there, to get a better understanding

Once we have our cache mechanism fully set up, and we can successfully move the changed stable files, this is essentially can be done at a later stage. The next step would be to try and identify the ways to minimise the frequency of library .c diff's yielding changes. Basically, when a Chapel user file has changed, the changes can easily fall into one of these 4 categories :

1) Cause a new instantiation of generic library function/type.
2) Cause a previously deadCode eliminated function to be used.
3) Reuse lib definition that was previously modified.
4) Only impact the application level code.

The first two changes would essentially mean trying to modify the .o for the library, and the two would fall into the category, where using the .o (for the library would be safe). The first two changes would cause recompilation and would essentially mean that we do not save any time during our make binary phase. So our main focus would be to try and work towards minimising case ``1`` . A potential way to fix case ``2`` would be to turn off dead-code elimination during --incremental, since dead code elimination would essentially mean removal of dead functions and instructions. So if we don't avoid having unused functions, then we can easily avoid recompilation. Also, it is quite common for a user to access library functions that they haven't used previously than having new generic instances of functions that they are already using.

So essentially we would want to have dead-code elimination turned off during --incremental compilation, however we would like to fine grain the way dead-code elimination works with --incremental. Since, we would still want the unused instructions to get removed, so this would require some more fine tuning. Once this is done, then we can integrate and move towards the function resolution and try to integrate our cache at the appropriate places so as to obtain a speedup.


Issues
------
* Currently the way splitting of header files is done is some slight issues in terms of performance. For instance the way the header file is generated with --incremental and with --no-incremental is different. Without the --incremental flag, the header file is a true header file and can be included in any of the user module,and hence allows for separate compilation for each of the file (generation of separate .o's which can be linked later on). But due to performance degradation after removing static keyword, the static keyword was added again to the normal compilation (without --incremental) which essentially means that these files are not header files in the true sense (since they cannot be essentially be included in multiple places). Another issue worth mentioning with the header file for --incremental is that included external header files, which are not true headers (contain function definitions , variable definitions etc in them) cause problem, and do not allow the inclusion of our header file into multiple places. Solving this problem may need a different approach to either the way the header file is split or and the way external header files are included.
* An issue with the approach suggested earlier is the part where we try to minimise the diff's b/w two runs of the same code. For smaller programs the main area where the programs differ is at the virtual method table. The reason behind this is the reordering of different groups of functions in the vmtable. Now one can suggest that the ideal way to go about this issue would be to try and sort the entries , however the entries in virtual method table do require an ordering (same functions should be at same locations for different modules). So the way we tried to solve this way to build up a custom sorting routine which takes into account the position of the previously encountered symbols with a similar name, but this again had some issues since the order in which we obtain the modules in codegen (after sorting) is different from the order in which we get it in functionResolution. This essentially then requires us to also first order the entries based on the modules. Currently, the work is being done to complete this entire sorting routine. Now once this is taken care of , there are some other problems for larger programs. For instance for larger programs apart from different modules there are also independent code blocks which switch their order. And the order in which different structs etc are present (which are independent) do also change their order. This part requires a more deeper understanding of why this happens, and as of now it is being looked into.
* As mentioned previously a part of our approach was trying to disable dead code elimination at later stages (so as to allow functions user to use another library function which he may not have used earlier) , however there is a drawback to this approach , since the way we have our function resolution and other checks is we only try to do further error handling (complete checks) on functions which actually get called in the code. So the user may try to include a new function , but it may so happen the during our previous analysis we skipped more thorough checks, and so when the user uses the function he may encounter an error (which essentially he shouldn't), and based on this issue we did try to find another approach, involving minimising the diff's and having a look at the difference and modifications that take place to a generic instance with change in parameters. Due to the issue mentioned earlier in trying to minimise the diff's , we still have to make progress along this line.
